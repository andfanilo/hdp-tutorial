{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to the Big Data ecosystem In this university tutorial, we will look into the Hadoop ecosystem by browsing through the Hortonworks Sandbox virtual machine. Expect to interact with: HDFS MapReduce / YARN Pig / Hive Flume Storm Zeppelin","title":"Introduction"},{"location":"#introduction-to-the-big-data-ecosystem","text":"In this university tutorial, we will look into the Hadoop ecosystem by browsing through the Hortonworks Sandbox virtual machine. Expect to interact with: HDFS MapReduce / YARN Pig / Hive Flume Storm Zeppelin","title":"Introduction to the Big Data ecosystem"},{"location":"prerequisites/","text":"Prerequisites Objectives Download a Hadoop virtual machine to use in VirtualBox Configure the memory used by the virtual machine to not kill our machine Start the virtual machine. Setup the environment Before working on the tutorial, we need a working Hadoop cluster. We are going to use: VirtualBox to run the virtual machine. Hortonworks Sandbox 2.5.0 . It is less demanding in terms of resources than version 3+ and sufficient for this tutorial. Also version 3+ removes some minor features used through the tutorial like the Pig View. Follow the given steps to import the machine in VirtualBox: Download Hortonworks Sandbox 2.5.0 and unzip the appliance for VirtualBox. Import the .ova file into VirtualBox. Don't start it yet if you want to configure it. You may configure the VM to use more or less RAM depending on your machine, through the Configuration > System view. The recommended value is around 6-8 Go RAM, but you should get away with using 2-4 Go. Start the virtual machine with the Start green arrow. This may take a few minutes. If the virtual machine stops during startup, it is generally because you don't have enough resources. Try to open a process manager and kill some RAM-consuming processes, or lower the RAM needed by the virtual machine using the above step. Open a web browser to http://localhost:8888 to be greeted with the Hortonworks Data Platform dashboard Recap We have imported the Hortonworks Data Platform into VirtualBox We configured the RAM the Virtual Machine will use for the tutorial We started the Virtual Machine and checked it runs correctly on our machine","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#objectives","text":"Download a Hadoop virtual machine to use in VirtualBox Configure the memory used by the virtual machine to not kill our machine Start the virtual machine.","title":"Objectives"},{"location":"prerequisites/#setup-the-environment","text":"Before working on the tutorial, we need a working Hadoop cluster. We are going to use: VirtualBox to run the virtual machine. Hortonworks Sandbox 2.5.0 . It is less demanding in terms of resources than version 3+ and sufficient for this tutorial. Also version 3+ removes some minor features used through the tutorial like the Pig View. Follow the given steps to import the machine in VirtualBox: Download Hortonworks Sandbox 2.5.0 and unzip the appliance for VirtualBox. Import the .ova file into VirtualBox. Don't start it yet if you want to configure it. You may configure the VM to use more or less RAM depending on your machine, through the Configuration > System view. The recommended value is around 6-8 Go RAM, but you should get away with using 2-4 Go. Start the virtual machine with the Start green arrow. This may take a few minutes. If the virtual machine stops during startup, it is generally because you don't have enough resources. Try to open a process manager and kill some RAM-consuming processes, or lower the RAM needed by the virtual machine using the above step. Open a web browser to http://localhost:8888 to be greeted with the Hortonworks Data Platform dashboard","title":"Setup the environment"},{"location":"prerequisites/#recap","text":"We have imported the Hortonworks Data Platform into VirtualBox We configured the RAM the Virtual Machine will use for the tutorial We started the Virtual Machine and checked it runs correctly on our machine","title":"Recap"},{"location":"resources/","text":"Resources Warning In construction","title":"Resources"},{"location":"resources/#resources","text":"Warning In construction","title":"Resources"},{"location":"td1/","text":"TD1 - First steps in the Hadoop ecosystem For this tutorial, we are going to focus on managing our cluster through Ambari and playing with some Hadoop features. Objectives Get used to the Ambari interface Upload data into HDFS - Hadoop Distributed File System Run MapReduce jobs on data in HDFS Run Pig jobs on data in HDFS Run Hive jobs on data in HDFS 1. Browsing the Ambari interface Open a web browser to http://localhost:8888 to be greeted with the Hortonworks Data Platform dashboard. Click on Launch Dashboard in the left column to pop-up a new browser to Ambari. You can also go to http://localhost:8080 directly. Enter the credentials: raj_ops / raj_ops . You will arrive on the Ambari dashboard, your cockpit into the Hadoop platform. Browsing the Ambari dashboard In the left sidebar, you should recognize some of the Hadoop services presented in the lecture. The main area displays KPIs to monitor the platform. Apart from Supervisors Live, there should not have too many red flags. The topbar has a few links to running services, list of hosts and an admin portal. Some links go to http://sandbox.hortonworks.com , replace that by http://localhost if you want to check them out. Browse the dashboard to answer the following: Questions on the Ambari dashboard How many Namenodes/Datanodes are currently running in the virtual machine ? What is the jdbc URL to connect to the Hive server ? The YARN cluster comprises of a Resource Manager and a Node Manager. How do you restart all Node Managers ? 2. Uploading files to HDFS There are two ways to upload data to HDFS: from the Ambari Files View and from a terminal. In this section we will: Upload the data folder of the project (which you can find here into HDFS through the Ambari Files View Move folders, then upload the titanic.csv file in HDFS with the command line. Using the Ambari Files View Question Find the Ambari Files View. Create a new folder root inside /user . Change the permissions of the folder /user/root to add group and other write/execute permissions. This will prove necessary so the root user can actually access its own folder from the command-line. Enter the /user/root folder, create a new data folder and upload geolocation.csv and trucks.csv inside. You should have /user/root/data/geolocation.csv and user/root/data/trucks.csv by the end. We now have data in HDFS! geolocation.csv \u2013 This is the collected geolocation data from the trucks. It contains records showing truck location, date, time, type of event, speed, etc. trucks.csv \u2013 This is data was exported from a relational database and it shows information on truck models, driverid, truckid, and aggregated mileage info. But what does it mean? You have to imagine the Hadoop Data Platform is actually a remote cluster of machines, so when you upload a big file in HDFS it gets cut into blocks of 64MB and spread accross multiple DataNodes, and the NameNode keeps a reference for this file in HDFS to all blocks in the cluster. Info The URL /user/root/data/geolocation.csv in Ambari Views is actually hdfs:///user/root/data/geolocation.csv . The hdfs:/// specifies to look into the HDFS cluster instead of locally when using a HDFS client. hdfs:/// is a shortcut for hdfs://<host>:<port>/ so you won't need to specify hdfs://sandbox.hortonworks.com:8020/ every time. Recap : we have uploaded data into HDFS, in the /user/root folder. Using the Command-line In this section, we will use the command-line to check that HDFS indeed has our data files in /user/root , then we will upload the titanic.csv . Tip Don't be discouraged by the command-line. It is one of the best ways to interact with remote systems, enabling scripting and copy-pasting commands! I recommend taking advantage of this session to follow the Directly ssh into the machine section, and download and try PuTTY on Windows, or use the terminal on Mac/Linux, to connect to remote machines like this virtual machine. This experience is especially helpful in enterprise when you need to run commands on remote machines. Directly ssh into the machine On Windows, use an ssh client like PuTTy , then SSH into localhost on port 2222. Credentials are root / hadoop . You can also, especially on Mac/Linux machines, open a terminal and directly ssh into the virtual machine with ssh -p 2222 root@localhost . Using the integrated browser-based shell You can connect to a shell in the Virtual machine with your browser in http://localhost:4200. Credentials are root / hadoop . Note In both cases you will be asked to change your root password. Type the current password again then change to a long password. Just remember it for future sessions . Moving the data folder to geoloc Now that you are connected to your virtual machine: You can access the hdfs command from the terminal. This should output the help from the command line. Display the version of HDFS with hdfs version . Output ssh -p 2222 root@localhost Could not create directory '/home/.../.ssh'. The authenticity of host '[localhost]:2222 ([127.0.0.1]:2222)' can't be established. Are you sure you want to continue connecting (yes/no)? yes Failed to add the host to the list of known hosts (/home/.../.ssh/known_hosts). root@localhost's password: Last login: Sun Nov 29 13:19:19 2020 from 10.0.2.2 [root@sandbox ~]# hdfs Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: dfs run a filesystem command on the file systems supported in Hadoop. classpath prints the classpath namenode -format format the DFS filesystem secondarynamenode run the DFS secondary namenode namenode run the DFS namenode journalnode run the DFS journalnode zkfc run the ZK Failover Controller daemon datanode run a DFS datanode dfsadmin run a DFS admin client envvars display computed Hadoop environment variables haadmin run a DFS HA admin client fsck run a DFS filesystem checking utility balancer run a cluster balancing utility jmxget get JMX exported values from NameNode or DataNode. mover run a utility to move block replicas across storage types oiv apply the offline fsimage viewer to an fsimage oiv_legacy apply the offline fsimage viewer to an legacy fsimage oev apply the offline edits viewer to an edits file fetchdt fetch a delegation token from the NameNode getconf get config values from configuration groups get the groups which users belong to snapshotDiff diff two snapshots of a directory or diff the current directory contents with a snapshot lsSnapshottableDir list all snapshottable dirs owned by the current user Use -help to see options portmap run a portmap service nfs3 run an NFS version 3 gateway cacheadmin configure the HDFS cache crypto configure HDFS encryption zones storagepolicies list/get/set block storage policies version print the version Most commands print help when invoked w/o parameters. [root@sandbox ~]# hdfs version Hadoop 2.7.3.2.5.0.0-1245 Subversion git@github.com:hortonworks/hadoop.git -r cb6e514b14fb60e9995e5ad9543315cd404b4e59 Compiled by jenkins on 2016-08-26T00:55Z Compiled with protoc 2.5.0 From source with checksum eba8ae32a1d8bb736a829d9dc18dddc2 This command was run using /usr/hdp/2.5.0.0-1245/hadoop/hadoop-common-2.7.3.2.5.0.0-1245.jar Tip The hdfs dfs command gives you access to all commands to interact with files in HDFS. Then hdfs dfs <command> -h gives you the command manual. List all folders inside HDFS with hdfs dfs -ls . The command hdfs dfs -ls will take you to hdfs:///user/root . hdfs uses your UNIX username to go to the HDFS home location. Since you're connected using root in the Virtual Machine, it connects by default to hdfs:///user/root . Rename the hdfs:///user/root/data folder to hdfs:///user/root/geoloc with hdfs dfs -mv . Remember by default that hdfs dfs -mv data geoloc is equivalent to hdfs dfs -mv hdfs:///user/root/data hdfs:///user/root/geoloc . Download the Titanic dataset into HDFS Use the wget command to download the Titanic dataset in the filesystem of your virtual machine. Verify the file is present with ls and correct with head -n 5 titanic.csv . Output [root@sandbox ~]# wget https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/titanic.csv --2020-11-29 14:13:59-- https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/titanic.csv Resolving raw.githubusercontent.com... 151.101.120.133 Connecting to raw.githubusercontent.com|151.101.120.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 60303 (59K) [text/plain] Saving to: \"titanic.csv\" 100%[==============================================================================================================================>] 60,303 --.-K/s in 0.008s 2020-11-29 14:13:59 (6.98 MB/s) - \"titanic.csv\" saved [60303/60303] [root@sandbox ~]# ls anaconda-ks.cfg blueprint.json build.out hdp install.log install.log.syslog sandbox.info start_ambari.sh start_hbase.sh titanic.csv [root@sandbox ~]# head -5 titanic.csv PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked 1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S 2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C 3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S 4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S To copy files from your local machine to HDFS, there is hdfs dfs -copyFromLocal <local_file> <path_in_HDFS> . Build the /user/root/test HDFS folder with hdfs dfs -mkdir <path_to_folder> . Copy titanic.csv file from the local VM into the /user/root/test HDFS folder. Changing permissions for hdfs:///tmp Currently, the hdfs:///tmp folder doesn't have permissions for everyone to write in. In the Hadoop ecosystem, root is not the superuser but hdfs is. So we need to be in the hdfs user before running set permissions. Run the following script. sudo su - su hdfs hdfs dfs -chmod -R 777 /tmp Don't forget to exit to go back to the root user. Recap In hdfs:///user/root/ , we have moved the contents of the data folder into geoloc We downloaded titanic.csv dataset into the virtual machine, then sent it to HDFS in hdfs:///user/root/test . We changed permissions for hdfs:///tmp . Output [root@sandbox ~]# hdfs dfs -ls /user/root/geoloc Found 2 items -rw-r--r-- 3 raj_ops hdfs 526677 2020-11-23 17:56 /user/root/geoloc/geolocation.csv -rw-r--r-- 3 raj_ops hdfs 61378 2020-11-23 17:56 /user/root/geoloc/trucks.csv [root@sandbox ~]# hdfs dfs -ls /user/root/test Found 1 items -rw-r--r-- 1 root hdfs 60301 2020-11-24 14:59 /user/root/test/titanic.csv [root@sandbox ~]# hdfs dfs -ls /tmp Found 6 items drwxrwxrwx - raj_ops hdfs 0 2020-11-27 16:10 /tmp/.pigjobs drwxrwxrwx - raj_ops hdfs 0 2020-11-27 16:09 /tmp/.pigscripts drwxrwxrwx - raj_ops hdfs 0 2020-11-27 16:09 /tmp/.pigstore drwxrwxrwx - hdfs hdfs 0 2016-10-25 07:48 /tmp/entity-file-history drwxrwxrwx - ambari-qa hdfs 0 2020-11-27 16:46 /tmp/hive drwx------ - root hdfs 0 2020-11-27 15:38 /tmp/temp890518890 Going back to our objectives Get used to the Ambari interface Upload data into HDFS - Hadoop Distributed File System Run MapReduce jobs on data in HDFS Run Pig jobs on data in HDFS Run Hive jobs on data in HDFS 3. Running a MapReduce job Time to compute stuff on data in HDFS. You should be using a command-line as the root user. Compute a distributed Pi Run the Pi example : yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi 4 100 . You can check all your jobs in the Resource Manager UI , from Ambari. Question Make the Pi calculation a bit more precise. Looking at the help of the function below, what would be nMaps ? nSamples ? [ root@sandbox ~ ] yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi Usage: org.apache.hadoop.examples.QuasiMonteCarlo <nMaps> <nSamples> Compute wordcount on files in HDFS Now we want to run a wordcount on a file inside HDFS, let's run it on files inside hdfs:///user/root/geoloc/ . Run yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount geoloc/geolocation.csv output . The command will not work if hdfs:///user/root/output already exists, in that case remove the folder with hdfs dfs -rm -r -f output . Examine the hdfs:///user/root/output folder. You can use hdfs dfs -ls output and hdfs dfs -cat output/part-r-00000 . Question Can you explain the part-r-00000 ? Only one reducer is working there. You can edit the number of reducers running with the flag -D mapred.reduce.tasks=10 . Edit the previous command to change the number of reducers working and output this in a new folder output2 . Question Examine the output2 folder. Can you note a difference with the previous execution ? Do you want to know more about the Java MapReduce code in the JAR? I am not going to have you write Java code to compute MapReduce, but in case you are wondering: import java.io.IOException ; import java.util.* ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.conf.* ; import org.apache.hadoop.io.* ; import org.apache.hadoop.mapreduce.* ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat ; public class WordCount { public static class Map extends Mapper < LongWritable , Text , Text , IntWritable > { private final static IntWritable one = new IntWritable ( 1 ); private Text word = new Text (); public void map ( LongWritable key , Text value , Context context ) throws IOException , InterruptedException { String line = value . toString (); StringTokenizer tokenizer = new StringTokenizer ( line ); while ( tokenizer . hasMoreTokens ()) { word . set ( tokenizer . nextToken ()); context . write ( word , one ); } } } public static class Reduce extends Reducer < Text , IntWritable , Text , IntWritable > { public void reduce ( Text key , Iterator < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = 0 ; while ( values . hasNext ()) { sum += values . next (). get (); } context . write ( key , new IntWritable ( sum )); } } public static void main ( String [] args ) throws Exception { Configuration conf = new Configuration (); Job job = new Job ( conf , \"wordcount\" ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); job . setMapperClass ( Map . class ); job . setReducerClass ( Reduce . class ); job . setInputFormatClass ( TextInputFormat . class ); job . setOutputFormatClass ( TextOutputFormat . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ] )); job . waitForCompletion ( true ); } } 4. Running Pig jobs If you take a look at the Java code for Wordcount, as BI Analysts / Data Scientists you are probably not going to want to write those. In this section we look into Pig, a higher-level tool which converts Pig Latin scripts into MapReduce scripts. There are two ways to start a Pig session here: Interactively through the Terminal/Command-line, by launching a Pig shell with pig and running commands one by one. There's a dedicated Pig View in Ambari. This should be the preferred method if you want to run full scripts but is much longer to run than the Pig shell. Sometimes it errors with an exception from Jetty, don't hesitate to close and come back to the View to reinitialize. For this section, we focus on using Pig to analyze the geolocation dataset. Run the following script/commands to load the file into a variable, then display the first ten lines from the geolocation file: geoloc = LOAD '/user/root/geoloc/geolocation.csv' USING PigStorage(',') AS (truckid,driverid,event,latitude,longitude,city,state,velocity,event_ind,idling_ind); geoloc_limit = LIMIT geoloc 10; DUMP geoloc_limit; Let's try to compute some stats on this file. geoloc = LOAD '/user/root/geoloc/geolocation.csv' USING PigStorage(',') AS (truckid:chararray, driverid:chararray, event:chararray, latitude:double, longitude:double, city:chararray, state:chararray, velocity:double, event_ind:long, idling_ind:long); truck_ids = GROUP geoloc BY truckid; result = FOREACH truck_ids GENERATE group AS truckid, COUNT(geoloc) as count; STORE result INTO '/tmp/results'; DUMP result; Question Check the hdfs:///tmp/results folder stored in HDFS by the STORE line. What can you say compared to the MapReduce wordcount ? Count the list of distinct cities visited per truckid Compute the mean velocity per truckid You may still find legacy Pig jobs in the nature, but you should not be required to write new Pig scripts anymore. 5. Running SQL jobs with Hive We as analysts are much more used to using SQL to process our data. The role of datawarehouse package in the Hadoop ecosystem goes to Hive. Note In today's world, SQL skills are still very important and one of the primary languages to manipulate data. So always work on your SQL. Like for Pig: you can start a Hive shell from your terminal/command-line, with the hive command. This is dedicated to simple SQL queries or operational management. There's a dedicated Hive View in Ambari, with some visualization capabilities. Time to create a table to analyze the geolocations again! Move the trucks.csv file outside of the hdfs:///user/root/geoloc folder to hdfs:///user/root/trucks . Create an external table for the hdfs:///user/root/geoloc folder which contains geolocation.csv . CREATE EXTERNAL TABLE geolocation ( truckid STRING , driverid STRING , event STRING , latitude DOUBLE , longitude DOUBLE , city STRING , state STRING , velocity DOUBLE , event_ind BIGINT , idling_ind BIGINT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/user/root/geoloc' ; Visualize the first rows of the table SELECT truckid FROM geolocation LIMIT 10 ; Note The way Hive works is every file inside the geoloc folder is read by Hive as data in the table. This is why we had to move out the trucks.csv file. The following command creates a Hive table pointing to a HDFS location. You can drop it, it won't destroy the data in HDFS Question Are you again able to count the list of distinct cities visited per truckid, and mean velocity per truckid ? On the Ambari View, count the number of distinct cities per trucks and display it on a bar chart. Conclusion We have seen the first Hadoop libraries, and we are now able to store raw data in HDFS, then process it with Pig or Hive. Going back to our objectives Get used to the Ambari interface Upload data into HDFS - Hadoop Distributed File System Run MapReduce jobs on data in HDFS Run Pig jobs on data in HDFS Run Hive jobs on data in HDFS In the next tutorial, we will build our own ETL-like solution in Big Data. We will have a look at ingesting unstructured data live to HDFS, extracting and structuring the important information into a Hive table, and build simple graphs.","title":"TD1 - First steps in the Hadoop ecosystem"},{"location":"td1/#td1-first-steps-in-the-hadoop-ecosystem","text":"For this tutorial, we are going to focus on managing our cluster through Ambari and playing with some Hadoop features.","title":"TD1 - First steps in the Hadoop ecosystem"},{"location":"td1/#objectives","text":"Get used to the Ambari interface Upload data into HDFS - Hadoop Distributed File System Run MapReduce jobs on data in HDFS Run Pig jobs on data in HDFS Run Hive jobs on data in HDFS","title":"Objectives"},{"location":"td1/#1-browsing-the-ambari-interface","text":"Open a web browser to http://localhost:8888 to be greeted with the Hortonworks Data Platform dashboard. Click on Launch Dashboard in the left column to pop-up a new browser to Ambari. You can also go to http://localhost:8080 directly. Enter the credentials: raj_ops / raj_ops . You will arrive on the Ambari dashboard, your cockpit into the Hadoop platform. Browsing the Ambari dashboard In the left sidebar, you should recognize some of the Hadoop services presented in the lecture. The main area displays KPIs to monitor the platform. Apart from Supervisors Live, there should not have too many red flags. The topbar has a few links to running services, list of hosts and an admin portal. Some links go to http://sandbox.hortonworks.com , replace that by http://localhost if you want to check them out. Browse the dashboard to answer the following: Questions on the Ambari dashboard How many Namenodes/Datanodes are currently running in the virtual machine ? What is the jdbc URL to connect to the Hive server ? The YARN cluster comprises of a Resource Manager and a Node Manager. How do you restart all Node Managers ?","title":"1. Browsing the Ambari interface"},{"location":"td1/#2-uploading-files-to-hdfs","text":"There are two ways to upload data to HDFS: from the Ambari Files View and from a terminal. In this section we will: Upload the data folder of the project (which you can find here into HDFS through the Ambari Files View Move folders, then upload the titanic.csv file in HDFS with the command line.","title":"2. Uploading files to HDFS"},{"location":"td1/#using-the-ambari-files-view","text":"Question Find the Ambari Files View. Create a new folder root inside /user . Change the permissions of the folder /user/root to add group and other write/execute permissions. This will prove necessary so the root user can actually access its own folder from the command-line. Enter the /user/root folder, create a new data folder and upload geolocation.csv and trucks.csv inside. You should have /user/root/data/geolocation.csv and user/root/data/trucks.csv by the end. We now have data in HDFS! geolocation.csv \u2013 This is the collected geolocation data from the trucks. It contains records showing truck location, date, time, type of event, speed, etc. trucks.csv \u2013 This is data was exported from a relational database and it shows information on truck models, driverid, truckid, and aggregated mileage info. But what does it mean? You have to imagine the Hadoop Data Platform is actually a remote cluster of machines, so when you upload a big file in HDFS it gets cut into blocks of 64MB and spread accross multiple DataNodes, and the NameNode keeps a reference for this file in HDFS to all blocks in the cluster. Info The URL /user/root/data/geolocation.csv in Ambari Views is actually hdfs:///user/root/data/geolocation.csv . The hdfs:/// specifies to look into the HDFS cluster instead of locally when using a HDFS client. hdfs:/// is a shortcut for hdfs://<host>:<port>/ so you won't need to specify hdfs://sandbox.hortonworks.com:8020/ every time. Recap : we have uploaded data into HDFS, in the /user/root folder.","title":"Using the Ambari Files View"},{"location":"td1/#using-the-command-line","text":"In this section, we will use the command-line to check that HDFS indeed has our data files in /user/root , then we will upload the titanic.csv . Tip Don't be discouraged by the command-line. It is one of the best ways to interact with remote systems, enabling scripting and copy-pasting commands! I recommend taking advantage of this session to follow the Directly ssh into the machine section, and download and try PuTTY on Windows, or use the terminal on Mac/Linux, to connect to remote machines like this virtual machine. This experience is especially helpful in enterprise when you need to run commands on remote machines. Directly ssh into the machine On Windows, use an ssh client like PuTTy , then SSH into localhost on port 2222. Credentials are root / hadoop . You can also, especially on Mac/Linux machines, open a terminal and directly ssh into the virtual machine with ssh -p 2222 root@localhost . Using the integrated browser-based shell You can connect to a shell in the Virtual machine with your browser in http://localhost:4200. Credentials are root / hadoop . Note In both cases you will be asked to change your root password. Type the current password again then change to a long password. Just remember it for future sessions . Moving the data folder to geoloc Now that you are connected to your virtual machine: You can access the hdfs command from the terminal. This should output the help from the command line. Display the version of HDFS with hdfs version . Output ssh -p 2222 root@localhost Could not create directory '/home/.../.ssh'. The authenticity of host '[localhost]:2222 ([127.0.0.1]:2222)' can't be established. Are you sure you want to continue connecting (yes/no)? yes Failed to add the host to the list of known hosts (/home/.../.ssh/known_hosts). root@localhost's password: Last login: Sun Nov 29 13:19:19 2020 from 10.0.2.2 [root@sandbox ~]# hdfs Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: dfs run a filesystem command on the file systems supported in Hadoop. classpath prints the classpath namenode -format format the DFS filesystem secondarynamenode run the DFS secondary namenode namenode run the DFS namenode journalnode run the DFS journalnode zkfc run the ZK Failover Controller daemon datanode run a DFS datanode dfsadmin run a DFS admin client envvars display computed Hadoop environment variables haadmin run a DFS HA admin client fsck run a DFS filesystem checking utility balancer run a cluster balancing utility jmxget get JMX exported values from NameNode or DataNode. mover run a utility to move block replicas across storage types oiv apply the offline fsimage viewer to an fsimage oiv_legacy apply the offline fsimage viewer to an legacy fsimage oev apply the offline edits viewer to an edits file fetchdt fetch a delegation token from the NameNode getconf get config values from configuration groups get the groups which users belong to snapshotDiff diff two snapshots of a directory or diff the current directory contents with a snapshot lsSnapshottableDir list all snapshottable dirs owned by the current user Use -help to see options portmap run a portmap service nfs3 run an NFS version 3 gateway cacheadmin configure the HDFS cache crypto configure HDFS encryption zones storagepolicies list/get/set block storage policies version print the version Most commands print help when invoked w/o parameters. [root@sandbox ~]# hdfs version Hadoop 2.7.3.2.5.0.0-1245 Subversion git@github.com:hortonworks/hadoop.git -r cb6e514b14fb60e9995e5ad9543315cd404b4e59 Compiled by jenkins on 2016-08-26T00:55Z Compiled with protoc 2.5.0 From source with checksum eba8ae32a1d8bb736a829d9dc18dddc2 This command was run using /usr/hdp/2.5.0.0-1245/hadoop/hadoop-common-2.7.3.2.5.0.0-1245.jar Tip The hdfs dfs command gives you access to all commands to interact with files in HDFS. Then hdfs dfs <command> -h gives you the command manual. List all folders inside HDFS with hdfs dfs -ls . The command hdfs dfs -ls will take you to hdfs:///user/root . hdfs uses your UNIX username to go to the HDFS home location. Since you're connected using root in the Virtual Machine, it connects by default to hdfs:///user/root . Rename the hdfs:///user/root/data folder to hdfs:///user/root/geoloc with hdfs dfs -mv . Remember by default that hdfs dfs -mv data geoloc is equivalent to hdfs dfs -mv hdfs:///user/root/data hdfs:///user/root/geoloc . Download the Titanic dataset into HDFS Use the wget command to download the Titanic dataset in the filesystem of your virtual machine. Verify the file is present with ls and correct with head -n 5 titanic.csv . Output [root@sandbox ~]# wget https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/titanic.csv --2020-11-29 14:13:59-- https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/titanic.csv Resolving raw.githubusercontent.com... 151.101.120.133 Connecting to raw.githubusercontent.com|151.101.120.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 60303 (59K) [text/plain] Saving to: \"titanic.csv\" 100%[==============================================================================================================================>] 60,303 --.-K/s in 0.008s 2020-11-29 14:13:59 (6.98 MB/s) - \"titanic.csv\" saved [60303/60303] [root@sandbox ~]# ls anaconda-ks.cfg blueprint.json build.out hdp install.log install.log.syslog sandbox.info start_ambari.sh start_hbase.sh titanic.csv [root@sandbox ~]# head -5 titanic.csv PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked 1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S 2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C 3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S 4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S To copy files from your local machine to HDFS, there is hdfs dfs -copyFromLocal <local_file> <path_in_HDFS> . Build the /user/root/test HDFS folder with hdfs dfs -mkdir <path_to_folder> . Copy titanic.csv file from the local VM into the /user/root/test HDFS folder. Changing permissions for hdfs:///tmp Currently, the hdfs:///tmp folder doesn't have permissions for everyone to write in. In the Hadoop ecosystem, root is not the superuser but hdfs is. So we need to be in the hdfs user before running set permissions. Run the following script. sudo su - su hdfs hdfs dfs -chmod -R 777 /tmp Don't forget to exit to go back to the root user. Recap In hdfs:///user/root/ , we have moved the contents of the data folder into geoloc We downloaded titanic.csv dataset into the virtual machine, then sent it to HDFS in hdfs:///user/root/test . We changed permissions for hdfs:///tmp . Output [root@sandbox ~]# hdfs dfs -ls /user/root/geoloc Found 2 items -rw-r--r-- 3 raj_ops hdfs 526677 2020-11-23 17:56 /user/root/geoloc/geolocation.csv -rw-r--r-- 3 raj_ops hdfs 61378 2020-11-23 17:56 /user/root/geoloc/trucks.csv [root@sandbox ~]# hdfs dfs -ls /user/root/test Found 1 items -rw-r--r-- 1 root hdfs 60301 2020-11-24 14:59 /user/root/test/titanic.csv [root@sandbox ~]# hdfs dfs -ls /tmp Found 6 items drwxrwxrwx - raj_ops hdfs 0 2020-11-27 16:10 /tmp/.pigjobs drwxrwxrwx - raj_ops hdfs 0 2020-11-27 16:09 /tmp/.pigscripts drwxrwxrwx - raj_ops hdfs 0 2020-11-27 16:09 /tmp/.pigstore drwxrwxrwx - hdfs hdfs 0 2016-10-25 07:48 /tmp/entity-file-history drwxrwxrwx - ambari-qa hdfs 0 2020-11-27 16:46 /tmp/hive drwx------ - root hdfs 0 2020-11-27 15:38 /tmp/temp890518890 Going back to our objectives Get used to the Ambari interface Upload data into HDFS - Hadoop Distributed File System Run MapReduce jobs on data in HDFS Run Pig jobs on data in HDFS Run Hive jobs on data in HDFS","title":"Using the Command-line"},{"location":"td1/#3-running-a-mapreduce-job","text":"Time to compute stuff on data in HDFS. You should be using a command-line as the root user.","title":"3. Running a MapReduce job"},{"location":"td1/#compute-a-distributed-pi","text":"Run the Pi example : yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi 4 100 . You can check all your jobs in the Resource Manager UI , from Ambari. Question Make the Pi calculation a bit more precise. Looking at the help of the function below, what would be nMaps ? nSamples ? [ root@sandbox ~ ] yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi Usage: org.apache.hadoop.examples.QuasiMonteCarlo <nMaps> <nSamples>","title":"Compute a distributed Pi"},{"location":"td1/#compute-wordcount-on-files-in-hdfs","text":"Now we want to run a wordcount on a file inside HDFS, let's run it on files inside hdfs:///user/root/geoloc/ . Run yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount geoloc/geolocation.csv output . The command will not work if hdfs:///user/root/output already exists, in that case remove the folder with hdfs dfs -rm -r -f output . Examine the hdfs:///user/root/output folder. You can use hdfs dfs -ls output and hdfs dfs -cat output/part-r-00000 . Question Can you explain the part-r-00000 ? Only one reducer is working there. You can edit the number of reducers running with the flag -D mapred.reduce.tasks=10 . Edit the previous command to change the number of reducers working and output this in a new folder output2 . Question Examine the output2 folder. Can you note a difference with the previous execution ? Do you want to know more about the Java MapReduce code in the JAR? I am not going to have you write Java code to compute MapReduce, but in case you are wondering: import java.io.IOException ; import java.util.* ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.conf.* ; import org.apache.hadoop.io.* ; import org.apache.hadoop.mapreduce.* ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat ; public class WordCount { public static class Map extends Mapper < LongWritable , Text , Text , IntWritable > { private final static IntWritable one = new IntWritable ( 1 ); private Text word = new Text (); public void map ( LongWritable key , Text value , Context context ) throws IOException , InterruptedException { String line = value . toString (); StringTokenizer tokenizer = new StringTokenizer ( line ); while ( tokenizer . hasMoreTokens ()) { word . set ( tokenizer . nextToken ()); context . write ( word , one ); } } } public static class Reduce extends Reducer < Text , IntWritable , Text , IntWritable > { public void reduce ( Text key , Iterator < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = 0 ; while ( values . hasNext ()) { sum += values . next (). get (); } context . write ( key , new IntWritable ( sum )); } } public static void main ( String [] args ) throws Exception { Configuration conf = new Configuration (); Job job = new Job ( conf , \"wordcount\" ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); job . setMapperClass ( Map . class ); job . setReducerClass ( Reduce . class ); job . setInputFormatClass ( TextInputFormat . class ); job . setOutputFormatClass ( TextOutputFormat . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ] )); job . waitForCompletion ( true ); } }","title":"Compute wordcount on files in HDFS"},{"location":"td1/#4-running-pig-jobs","text":"If you take a look at the Java code for Wordcount, as BI Analysts / Data Scientists you are probably not going to want to write those. In this section we look into Pig, a higher-level tool which converts Pig Latin scripts into MapReduce scripts. There are two ways to start a Pig session here: Interactively through the Terminal/Command-line, by launching a Pig shell with pig and running commands one by one. There's a dedicated Pig View in Ambari. This should be the preferred method if you want to run full scripts but is much longer to run than the Pig shell. Sometimes it errors with an exception from Jetty, don't hesitate to close and come back to the View to reinitialize. For this section, we focus on using Pig to analyze the geolocation dataset. Run the following script/commands to load the file into a variable, then display the first ten lines from the geolocation file: geoloc = LOAD '/user/root/geoloc/geolocation.csv' USING PigStorage(',') AS (truckid,driverid,event,latitude,longitude,city,state,velocity,event_ind,idling_ind); geoloc_limit = LIMIT geoloc 10; DUMP geoloc_limit; Let's try to compute some stats on this file. geoloc = LOAD '/user/root/geoloc/geolocation.csv' USING PigStorage(',') AS (truckid:chararray, driverid:chararray, event:chararray, latitude:double, longitude:double, city:chararray, state:chararray, velocity:double, event_ind:long, idling_ind:long); truck_ids = GROUP geoloc BY truckid; result = FOREACH truck_ids GENERATE group AS truckid, COUNT(geoloc) as count; STORE result INTO '/tmp/results'; DUMP result; Question Check the hdfs:///tmp/results folder stored in HDFS by the STORE line. What can you say compared to the MapReduce wordcount ? Count the list of distinct cities visited per truckid Compute the mean velocity per truckid You may still find legacy Pig jobs in the nature, but you should not be required to write new Pig scripts anymore.","title":"4. Running Pig jobs"},{"location":"td1/#5-running-sql-jobs-with-hive","text":"We as analysts are much more used to using SQL to process our data. The role of datawarehouse package in the Hadoop ecosystem goes to Hive. Note In today's world, SQL skills are still very important and one of the primary languages to manipulate data. So always work on your SQL. Like for Pig: you can start a Hive shell from your terminal/command-line, with the hive command. This is dedicated to simple SQL queries or operational management. There's a dedicated Hive View in Ambari, with some visualization capabilities. Time to create a table to analyze the geolocations again! Move the trucks.csv file outside of the hdfs:///user/root/geoloc folder to hdfs:///user/root/trucks . Create an external table for the hdfs:///user/root/geoloc folder which contains geolocation.csv . CREATE EXTERNAL TABLE geolocation ( truckid STRING , driverid STRING , event STRING , latitude DOUBLE , longitude DOUBLE , city STRING , state STRING , velocity DOUBLE , event_ind BIGINT , idling_ind BIGINT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/user/root/geoloc' ; Visualize the first rows of the table SELECT truckid FROM geolocation LIMIT 10 ; Note The way Hive works is every file inside the geoloc folder is read by Hive as data in the table. This is why we had to move out the trucks.csv file. The following command creates a Hive table pointing to a HDFS location. You can drop it, it won't destroy the data in HDFS Question Are you again able to count the list of distinct cities visited per truckid, and mean velocity per truckid ? On the Ambari View, count the number of distinct cities per trucks and display it on a bar chart.","title":"5. Running SQL jobs with Hive"},{"location":"td1/#conclusion","text":"We have seen the first Hadoop libraries, and we are now able to store raw data in HDFS, then process it with Pig or Hive. Going back to our objectives Get used to the Ambari interface Upload data into HDFS - Hadoop Distributed File System Run MapReduce jobs on data in HDFS Run Pig jobs on data in HDFS Run Hive jobs on data in HDFS In the next tutorial, we will build our own ETL-like solution in Big Data. We will have a look at ingesting unstructured data live to HDFS, extracting and structuring the important information into a Hive table, and build simple graphs.","title":"Conclusion"},{"location":"td2/","text":"TD2 - Simulating customer behavior analytics in ecommerce Log analysis is one of the first use cases enabled by Big Data Processing, from parsing web crawlers logs to analyzing customer behavior on websites by rebuilding their sessions from Apache logs. In this practice session, we will replicate a (albeit smaller) Big Data pipeline to collect and visualize Apache logs. Saving some memory using Terminal This tutorial makes heavy use of Hive and Zeppelin to process data. If you are using less than 8 Go of RAM for the virtual machine, try to not use Ambari for this session and only use the terminal to upload and manage data in HDFS. Ambari consumes a lot of memory when acessed so this saves some resources. Objectives Structuring Apache logs with Hive and regex Ingesting Apache logs into HDFS in realtime with Flume Building a data dashboard with Zeppelin 1. Structuring Apache logs with Hive and regex Upload data to HDFS Before building the whole pipeline, let's have a look at a sample of Apache logs. 164.29.239.18 - - [01/Aug/2014:11:48:59 -0400] \"GET /department/apparel/products HTTP/1.1\" 200 991 \"-\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36\" A sample of Apache logs is available here in the access.log.2.zip file. We will upload this data into HDFS, parse it using an external Hive table over it and run some SQL queries. Download and unzip the folder, locally or in your virtual machine depending on how you want to upload the data in HDFS. Upload the data in HDFS, at the location /user/root/access . You should end with /user/root/access/access.log.2 . Help on terminal [ root@sandbox ~ ] # hdfs dfs -mkdir -p /user/root/access [ root@sandbox ~ ] # wget https://github.com/andfanilo/hdp-tutorial/raw/main/data/access.log.2.zip --2020-12-05 14 :49:52-- https://github.com/andfanilo/hdp-tutorial/raw/main/data/access.log.2.zip Resolving github.com... 140 .82.121.4 Connecting to github.com | 140 .82.121.4 | :443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/access.log.2.zip [ following ] --2020-12-05 14 :49:52-- https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/access.log.2.zip Resolving raw.githubusercontent.com... 151 .101.120.133 Connecting to raw.githubusercontent.com | 151 .101.120.133 | :443... connected. HTTP request sent, awaiting response... 200 OK Length: 3224097 ( 3 .1M ) [ application/zip ] Saving to: \"access.log.2.zip\" 100 % [============================================================================================================================== > ] 3 ,224,097 14 .4M/s in 0 .2s 2020 -12-05 14 :49:53 ( 14 .4 MB/s ) - \"access.log.2.zip\" saved [ 3224097 /3224097 ] [ root@sandbox ~ ] # unzip access.log.2.zip Archive: access.log.2.zip inflating: access.log.2 [ root@sandbox ~ ] # hdfs dfs -copyFromLocal access.log.2 /user/root/access Take a look at the end of the file in HDFS, using the tail command in HDFS in the terminal. Output [ root@sandbox ~ ] # hdfs dfs -tail access/access.log.2 6 .1 ; WOW64 ; rv:30.0 ) Gecko/20100101 Firefox/30.0 \" 64.232.194.248 - - [14/Jun/2014:23:43:32 -0400] \" GET /support HTTP/1.1 \" 200 887 \" - \" \" Mozilla/5.0 ( Windows NT 6 .1 ; rv:30.0 ) Gecko/20100101 Firefox/30.0 \" 138.9.185.141 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/golf HTTP/1.1 \" 200 1075 \" - \" \" Mozilla/5.0 ( Windows NT 6 .3 ; WOW64 ) AppleWebKit/537.36 ( KHTML, like Gecko ) Chrome/35.0.1916.153 Safari/537.36 \" 152.208.225.65 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/golf HTTP/1.1 \" 200 1358 \" - \" \" Mozilla/5.0 ( Windows NT 6 .1 ) AppleWebKit/537.36 ( KHTML, like Gecko ) Chrome/35.0.1916.153 Safari/537.36 \" 84.246.94.164 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/fitness/category/tennis%20 & %20racquet HTTP/1.1 \" 200 907 \" - \" \" Mozilla/5.0 ( Windows NT 6 .1 ; WOW64 ; rv:30.0 ) Gecko/20100101 Firefox/30.0 \" 167.228.157.189 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/outdoors HTTP/1.1 \" 200 2166 \" - \" \" Mozilla/5.0 ( Macintosh ; Intel Mac OS X 10_9_3 ) AppleWebKit/537.36 ( KHTML, like Gecko ) Chrome/35.0.1916.153 Safari/537.36 \" Build a Hive table over the log file In the previous tutorial, we created a Hive table over CSV files using the keywords ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' . The format of the file in HDFS, which Hive will parse on-demand, must be specified at table creation. Here we will use a regex to extract all the information we need from the log files. To enable parsing files in HDFS using regex with Hive, we use a specific SerDe (for serializer/deserializer) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' . This SERDE is not provided by default, we will need to register a hive-contrib.jar plugin which contains the class. Otherwise you will get Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.contrib.serde2.RegexSerDe not found exceptions. Let's try this: Open a terminal to your Virtual Machine. Open a Hive command line: hive . Add the hive-contrib.jar JAR: ADD JAR / usr / hdp / current / hive - client / lib / hive - contrib . jar ; Create an external Hive table intermediate_access_logs (copy-paste the following command): CREATE EXTERNAL TABLE intermediate_access_logs ( ip STRING , log_date STRING , method STRING , url_site STRING , http_version STRING , code1 STRING , code2 STRING , dash STRING , user_agent STRING ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'input.regex' = '([^ ]*) - - \\\\[([^\\\\]]*)\\\\] \"([^\\ ]*) ([^\\ ]*) ([^\\ ]*)\" (\\\\d*) (\\\\d*) \"([^\"]*)\" \"([^\"]*)\"' , 'output.format.string' = \"%1$$s %2$$s %3$$s %4$$s %5$$s %6$$s %7$$s %8$$s %9$$s\" ) LOCATION '/user/root/access' ; Now whenever you run a SQL query on intermediate_access_logs , Hive will run a MapReduce job by first parsing all files in the /user/root/access with the provided regex, then run your query. Output hive> DESCRIBE intermediate_access_logs; OK ip string from deserializer log_date string from deserializer method string from deserializer url_site string from deserializer http_version string from deserializer code1 string from deserializer code2 string from deserializer dash string from deserializer user_agent string from deserializer Time taken: 0.997 seconds, Fetched: 9 row(s) hive> SELECT ip, log_date, user_agent from intermediate_access_logs LIMIT 5; OK 79.133.215.123 14/Jun/2014:10:30:13 -0400 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 162.235.161.200 14/Jun/2014:10:30:13 -0400 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.76.4 (KHTML, like Gecko) Version/7.0.4 Safari/537.76.4 39.244.91.133 14/Jun/2014:10:30:14 -0400 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 150.47.54.136 14/Jun/2014:10:30:14 -0400 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 217.89.36.129 14/Jun/2014:10:30:14 -0400 Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0 Time taken: 0.29 seconds, Fetched: 5 row(s) hive> Running a query on intermediate_access_logs will parse files with regex every time, which is time consuming. Create a new clean_access_logs table with the output for intermediate_access_logs as content. We also optimize the table storage with the ORC format . Using ORC files improves performance when Hive is reading, writing, and processing data. CREATE TABLE clean_access_logs STORED AS ORC AS SELECT * FROM intermediate_access_logs ; Also build a smaller table so you can experiment on it before running on the full dataset: CREATE TABLE sample_access_logs STORED AS ORC AS SELECT * FROM clean_access_logs LIMIT 1000 ; You're now free to work on the clean_access_logs or sample_access_logs tables SQL questions Choose 1-2 questions to try: Can you count the number of occurences for each IP address ? Display how many times each product has been bought What percentage of IP addresses went to checkout their basket ? If you case the date as a Date you should be able to build a web journey of an IP address on the website. For all IP adresses that went to checkout, compute the number of products each has bought before. Zeppelin, the Big Data notebook Apache Zeppelin is a Web-based notebook for interactive data analytics and collaborative documents. You can plugin multiple interpreters to run different Big Data engine inside, by default Hive JDBC and Spark are already configured to run. Open http://localhost:9995 for a first peek at Zeppelin. Create a new note, write some Markdown in the first cell with %md as a first line to choose the Markdown interpreter, and run the cell: See what happens when you toggle the default view to simple and report . Each cell has its own set of settings too. For example, change the width of the first cell to 6: Create a new cell, set the interpreter to Hive JDBC with %jdbc(hive) and run a SQL query against clean_access_logs again. Do you see that the Hive JDBC cell results have a toolbar for displaying graphs? Play with it a bit then try to replicate the following notebook in report view, with a cell for counting the number of occurences for a few ip addresses. Recap We loaded raw Apache logs into HDFS We parsed them by pointing an Hive Table over the logs with a regex deserializer to parse each line We saved Hive tables with the extracted info and optimized with ORC. We built a Zeppelin dashboard with some info on the contents of the log. Going back to our objectives Structuring Apache logs with Hive and regex Ingesting Apache logs into HDFS in realtime with Flume Building a data dashboard with Zeppelin 2. Generating logs with Python Copy gen_logs to VM Run Python simulation 3. Ingesting data in HDFS with Flume Configure Flume Output in external Hive table 4. Dashboarding with Zeppelin TODO Conclusion","title":"TD2 - Simulating customer behavior analytics in ecommerce"},{"location":"td2/#td2-simulating-customer-behavior-analytics-in-ecommerce","text":"Log analysis is one of the first use cases enabled by Big Data Processing, from parsing web crawlers logs to analyzing customer behavior on websites by rebuilding their sessions from Apache logs. In this practice session, we will replicate a (albeit smaller) Big Data pipeline to collect and visualize Apache logs. Saving some memory using Terminal This tutorial makes heavy use of Hive and Zeppelin to process data. If you are using less than 8 Go of RAM for the virtual machine, try to not use Ambari for this session and only use the terminal to upload and manage data in HDFS. Ambari consumes a lot of memory when acessed so this saves some resources.","title":"TD2 - Simulating customer behavior analytics in ecommerce"},{"location":"td2/#objectives","text":"Structuring Apache logs with Hive and regex Ingesting Apache logs into HDFS in realtime with Flume Building a data dashboard with Zeppelin","title":"Objectives"},{"location":"td2/#1-structuring-apache-logs-with-hive-and-regex","text":"","title":"1. Structuring Apache logs with Hive and regex"},{"location":"td2/#upload-data-to-hdfs","text":"Before building the whole pipeline, let's have a look at a sample of Apache logs. 164.29.239.18 - - [01/Aug/2014:11:48:59 -0400] \"GET /department/apparel/products HTTP/1.1\" 200 991 \"-\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36\" A sample of Apache logs is available here in the access.log.2.zip file. We will upload this data into HDFS, parse it using an external Hive table over it and run some SQL queries. Download and unzip the folder, locally or in your virtual machine depending on how you want to upload the data in HDFS. Upload the data in HDFS, at the location /user/root/access . You should end with /user/root/access/access.log.2 . Help on terminal [ root@sandbox ~ ] # hdfs dfs -mkdir -p /user/root/access [ root@sandbox ~ ] # wget https://github.com/andfanilo/hdp-tutorial/raw/main/data/access.log.2.zip --2020-12-05 14 :49:52-- https://github.com/andfanilo/hdp-tutorial/raw/main/data/access.log.2.zip Resolving github.com... 140 .82.121.4 Connecting to github.com | 140 .82.121.4 | :443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/access.log.2.zip [ following ] --2020-12-05 14 :49:52-- https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/access.log.2.zip Resolving raw.githubusercontent.com... 151 .101.120.133 Connecting to raw.githubusercontent.com | 151 .101.120.133 | :443... connected. HTTP request sent, awaiting response... 200 OK Length: 3224097 ( 3 .1M ) [ application/zip ] Saving to: \"access.log.2.zip\" 100 % [============================================================================================================================== > ] 3 ,224,097 14 .4M/s in 0 .2s 2020 -12-05 14 :49:53 ( 14 .4 MB/s ) - \"access.log.2.zip\" saved [ 3224097 /3224097 ] [ root@sandbox ~ ] # unzip access.log.2.zip Archive: access.log.2.zip inflating: access.log.2 [ root@sandbox ~ ] # hdfs dfs -copyFromLocal access.log.2 /user/root/access Take a look at the end of the file in HDFS, using the tail command in HDFS in the terminal. Output [ root@sandbox ~ ] # hdfs dfs -tail access/access.log.2 6 .1 ; WOW64 ; rv:30.0 ) Gecko/20100101 Firefox/30.0 \" 64.232.194.248 - - [14/Jun/2014:23:43:32 -0400] \" GET /support HTTP/1.1 \" 200 887 \" - \" \" Mozilla/5.0 ( Windows NT 6 .1 ; rv:30.0 ) Gecko/20100101 Firefox/30.0 \" 138.9.185.141 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/golf HTTP/1.1 \" 200 1075 \" - \" \" Mozilla/5.0 ( Windows NT 6 .3 ; WOW64 ) AppleWebKit/537.36 ( KHTML, like Gecko ) Chrome/35.0.1916.153 Safari/537.36 \" 152.208.225.65 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/golf HTTP/1.1 \" 200 1358 \" - \" \" Mozilla/5.0 ( Windows NT 6 .1 ) AppleWebKit/537.36 ( KHTML, like Gecko ) Chrome/35.0.1916.153 Safari/537.36 \" 84.246.94.164 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/fitness/category/tennis%20 & %20racquet HTTP/1.1 \" 200 907 \" - \" \" Mozilla/5.0 ( Windows NT 6 .1 ; WOW64 ; rv:30.0 ) Gecko/20100101 Firefox/30.0 \" 167.228.157.189 - - [14/Jun/2014:23:43:32 -0400] \" GET /department/outdoors HTTP/1.1 \" 200 2166 \" - \" \" Mozilla/5.0 ( Macintosh ; Intel Mac OS X 10_9_3 ) AppleWebKit/537.36 ( KHTML, like Gecko ) Chrome/35.0.1916.153 Safari/537.36 \"","title":"Upload data to HDFS"},{"location":"td2/#build-a-hive-table-over-the-log-file","text":"In the previous tutorial, we created a Hive table over CSV files using the keywords ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' . The format of the file in HDFS, which Hive will parse on-demand, must be specified at table creation. Here we will use a regex to extract all the information we need from the log files. To enable parsing files in HDFS using regex with Hive, we use a specific SerDe (for serializer/deserializer) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' . This SERDE is not provided by default, we will need to register a hive-contrib.jar plugin which contains the class. Otherwise you will get Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.contrib.serde2.RegexSerDe not found exceptions. Let's try this: Open a terminal to your Virtual Machine. Open a Hive command line: hive . Add the hive-contrib.jar JAR: ADD JAR / usr / hdp / current / hive - client / lib / hive - contrib . jar ; Create an external Hive table intermediate_access_logs (copy-paste the following command): CREATE EXTERNAL TABLE intermediate_access_logs ( ip STRING , log_date STRING , method STRING , url_site STRING , http_version STRING , code1 STRING , code2 STRING , dash STRING , user_agent STRING ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'input.regex' = '([^ ]*) - - \\\\[([^\\\\]]*)\\\\] \"([^\\ ]*) ([^\\ ]*) ([^\\ ]*)\" (\\\\d*) (\\\\d*) \"([^\"]*)\" \"([^\"]*)\"' , 'output.format.string' = \"%1$$s %2$$s %3$$s %4$$s %5$$s %6$$s %7$$s %8$$s %9$$s\" ) LOCATION '/user/root/access' ; Now whenever you run a SQL query on intermediate_access_logs , Hive will run a MapReduce job by first parsing all files in the /user/root/access with the provided regex, then run your query. Output hive> DESCRIBE intermediate_access_logs; OK ip string from deserializer log_date string from deserializer method string from deserializer url_site string from deserializer http_version string from deserializer code1 string from deserializer code2 string from deserializer dash string from deserializer user_agent string from deserializer Time taken: 0.997 seconds, Fetched: 9 row(s) hive> SELECT ip, log_date, user_agent from intermediate_access_logs LIMIT 5; OK 79.133.215.123 14/Jun/2014:10:30:13 -0400 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 162.235.161.200 14/Jun/2014:10:30:13 -0400 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.76.4 (KHTML, like Gecko) Version/7.0.4 Safari/537.76.4 39.244.91.133 14/Jun/2014:10:30:14 -0400 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 150.47.54.136 14/Jun/2014:10:30:14 -0400 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 217.89.36.129 14/Jun/2014:10:30:14 -0400 Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0 Time taken: 0.29 seconds, Fetched: 5 row(s) hive> Running a query on intermediate_access_logs will parse files with regex every time, which is time consuming. Create a new clean_access_logs table with the output for intermediate_access_logs as content. We also optimize the table storage with the ORC format . Using ORC files improves performance when Hive is reading, writing, and processing data. CREATE TABLE clean_access_logs STORED AS ORC AS SELECT * FROM intermediate_access_logs ; Also build a smaller table so you can experiment on it before running on the full dataset: CREATE TABLE sample_access_logs STORED AS ORC AS SELECT * FROM clean_access_logs LIMIT 1000 ; You're now free to work on the clean_access_logs or sample_access_logs tables SQL questions Choose 1-2 questions to try: Can you count the number of occurences for each IP address ? Display how many times each product has been bought What percentage of IP addresses went to checkout their basket ? If you case the date as a Date you should be able to build a web journey of an IP address on the website. For all IP adresses that went to checkout, compute the number of products each has bought before.","title":"Build a Hive table over the log file"},{"location":"td2/#zeppelin-the-big-data-notebook","text":"Apache Zeppelin is a Web-based notebook for interactive data analytics and collaborative documents. You can plugin multiple interpreters to run different Big Data engine inside, by default Hive JDBC and Spark are already configured to run. Open http://localhost:9995 for a first peek at Zeppelin. Create a new note, write some Markdown in the first cell with %md as a first line to choose the Markdown interpreter, and run the cell: See what happens when you toggle the default view to simple and report . Each cell has its own set of settings too. For example, change the width of the first cell to 6: Create a new cell, set the interpreter to Hive JDBC with %jdbc(hive) and run a SQL query against clean_access_logs again. Do you see that the Hive JDBC cell results have a toolbar for displaying graphs? Play with it a bit then try to replicate the following notebook in report view, with a cell for counting the number of occurences for a few ip addresses. Recap We loaded raw Apache logs into HDFS We parsed them by pointing an Hive Table over the logs with a regex deserializer to parse each line We saved Hive tables with the extracted info and optimized with ORC. We built a Zeppelin dashboard with some info on the contents of the log. Going back to our objectives Structuring Apache logs with Hive and regex Ingesting Apache logs into HDFS in realtime with Flume Building a data dashboard with Zeppelin","title":"Zeppelin, the Big Data notebook"},{"location":"td2/#2-generating-logs-with-python","text":"Copy gen_logs to VM Run Python simulation","title":"2. Generating logs with Python"},{"location":"td2/#3-ingesting-data-in-hdfs-with-flume","text":"Configure Flume Output in external Hive table","title":"3. Ingesting data in HDFS with Flume"},{"location":"td2/#4-dashboarding-with-zeppelin","text":"TODO","title":"4. Dashboarding with Zeppelin"},{"location":"td2/#conclusion","text":"","title":"Conclusion"},{"location":"td3/","text":"TD3 Warning In construction","title":"TD3"},{"location":"td3/#td3","text":"Warning In construction","title":"TD3"}]}