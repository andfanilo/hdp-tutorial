
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.1.6">
    
    
      
        <title>TD2 - Simulating customer behavior analytics in ecommerce - Big Data Tutorial - Hortonworks Data Platform</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6910b76c.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.196e0c26.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
    
    
    
      
        
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="indigo" data-md-color-accent="">
      
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#td2-simulating-customer-behavior-analytics-in-ecommerce" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="Big Data Tutorial - Hortonworks Data Platform" class="md-header-nav__button md-logo" aria-label="Big Data Tutorial - Hortonworks Data Platform">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Big Data Tutorial - Hortonworks Data Platform
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              TD2 - Simulating customer behavior analytics in ecommerce
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Big Data Tutorial - Hortonworks Data Platform" class="md-nav__button md-logo" aria-label="Big Data Tutorial - Hortonworks Data Platform">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Big Data Tutorial - Hortonworks Data Platform
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." class="md-nav__link">
      Introduction
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../prerequisites/" class="md-nav__link">
      Prerequisites
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../td1/" class="md-nav__link">
      TD1 - First steps in the Hadoop ecosystem
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        TD2 - Simulating customer behavior analytics in ecommerce
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      TD2 - Simulating customer behavior analytics in ecommerce
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objectives" class="md-nav__link">
    Objectives
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-structuring-apache-logs-with-hive-and-regex" class="md-nav__link">
    1. Structuring Apache logs with Hive and regex
  </a>
  
    <nav class="md-nav" aria-label="1. Structuring Apache logs with Hive and regex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#upload-data-to-hdfs" class="md-nav__link">
    Upload data to HDFS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build-a-hive-table-over-the-log-file" class="md-nav__link">
    Build a Hive table over the log file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zeppelin-the-big-data-notebook" class="md-nav__link">
    Zeppelin, the Big Data notebook
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-generating-logs-with-python" class="md-nav__link">
    2. Generating logs with Python
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-ingesting-data-in-hdfs-with-flume" class="md-nav__link">
    3. Ingesting data in HDFS with Flume
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-dashboarding-with-zeppelin" class="md-nav__link">
    4. Dashboarding with Zeppelin
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../td3/" class="md-nav__link">
      TD3
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../resources/" class="md-nav__link">
      Resources
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objectives" class="md-nav__link">
    Objectives
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-structuring-apache-logs-with-hive-and-regex" class="md-nav__link">
    1. Structuring Apache logs with Hive and regex
  </a>
  
    <nav class="md-nav" aria-label="1. Structuring Apache logs with Hive and regex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#upload-data-to-hdfs" class="md-nav__link">
    Upload data to HDFS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build-a-hive-table-over-the-log-file" class="md-nav__link">
    Build a Hive table over the log file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zeppelin-the-big-data-notebook" class="md-nav__link">
    Zeppelin, the Big Data notebook
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-generating-logs-with-python" class="md-nav__link">
    2. Generating logs with Python
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-ingesting-data-in-hdfs-with-flume" class="md-nav__link">
    3. Ingesting data in HDFS with Flume
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-dashboarding-with-zeppelin" class="md-nav__link">
    4. Dashboarding with Zeppelin
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="td2-simulating-customer-behavior-analytics-in-ecommerce">TD2 - Simulating customer behavior analytics in ecommerce</h1>
<p>Log analysis is one of the first use cases enabled by Big Data Processing, from parsing web crawlers logs to analyzing customer behavior on websites by rebuilding their sessions from Apache logs.</p>
<p><img alt="" src="../images/apache-logs.png" /></p>
<p>In this practice session, we will replicate a (albeit smaller) Big Data pipeline to collect and visualize Apache logs.</p>
<p><img alt="" src="../images/td2-overview.png" /></p>
<div class="admonition warning">
<p class="admonition-title">Saving some memory using Terminal</p>
<p>This tutorial makes heavy use of Hive and Zeppelin to process data. If you are using less than 8 Go of RAM for the virtual machine, try to not use Ambari for this session and only use the terminal to upload and manage data in HDFS. Ambari consumes a lot of memory when acessed so this saves some resources.    </p>
</div>
<h2 id="objectives">Objectives</h2>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Structuring Apache logs with Hive and regex</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Ingesting Apache logs into HDFS in realtime with Flume</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Building a data dashboard with Zeppelin</li>
</ul>
<h2 id="1-structuring-apache-logs-with-hive-and-regex">1. Structuring Apache logs with Hive and regex</h2>
<h3 id="upload-data-to-hdfs">Upload data to HDFS</h3>
<p>Before building the whole pipeline, let's have a look at a sample of Apache logs.</p>
<div class="highlight"><pre><span></span><code>164.29.239.18 - - [01/Aug/2014:11:48:59 -0400] &quot;GET /department/apparel/products HTTP/1.1&quot; 200 991 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36&quot;
</code></pre></div>
<p>A sample of Apache logs is available <a href="https://github.com/andfanilo/hdp-tutorial/tree/main/data">here</a> in the <code>access.log.2.zip</code> file. We will upload this data into HDFS, parse it using an external Hive table over it and run some SQL queries.</p>
<ul>
<li>Download and unzip the folder, locally or in your virtual machine depending on how you want to upload the data in HDFS.</li>
<li>Upload the data in HDFS, at the location <code>/user/root/access</code>. You should end with <code>/user/root/access/access.log.2</code>.</li>
</ul>
<details class="tip"><summary>Help on terminal</summary><div class="highlight"><pre><span></span><code><span class="o">[</span>root@sandbox ~<span class="o">]</span><span class="c1"># hdfs dfs -mkdir -p /user/root/access</span>
<span class="o">[</span>root@sandbox ~<span class="o">]</span><span class="c1"># wget https://github.com/andfanilo/hdp-tutorial/raw/main/data/access.log.2.zip</span>
--2020-12-05 <span class="m">14</span>:49:52--  https://github.com/andfanilo/hdp-tutorial/raw/main/data/access.log.2.zip
Resolving github.com... <span class="m">140</span>.82.121.4
Connecting to github.com<span class="p">|</span><span class="m">140</span>.82.121.4<span class="p">|</span>:443... connected.
HTTP request sent, awaiting response... <span class="m">302</span> Found
Location: https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/access.log.2.zip <span class="o">[</span>following<span class="o">]</span>
--2020-12-05 <span class="m">14</span>:49:52--  https://raw.githubusercontent.com/andfanilo/hdp-tutorial/main/data/access.log.2.zip
Resolving raw.githubusercontent.com... <span class="m">151</span>.101.120.133
Connecting to raw.githubusercontent.com<span class="p">|</span><span class="m">151</span>.101.120.133<span class="p">|</span>:443... connected.
HTTP request sent, awaiting response... <span class="m">200</span> OK
Length: <span class="m">3224097</span> <span class="o">(</span><span class="m">3</span>.1M<span class="o">)</span> <span class="o">[</span>application/zip<span class="o">]</span>
Saving to: <span class="s2">&quot;access.log.2.zip&quot;</span>

<span class="m">100</span>%<span class="o">[==============================================================================================================================</span>&gt;<span class="o">]</span> <span class="m">3</span>,224,097   <span class="m">14</span>.4M/s   in <span class="m">0</span>.2s

<span class="m">2020</span>-12-05 <span class="m">14</span>:49:53 <span class="o">(</span><span class="m">14</span>.4 MB/s<span class="o">)</span> - <span class="s2">&quot;access.log.2.zip&quot;</span> saved <span class="o">[</span><span class="m">3224097</span>/3224097<span class="o">]</span>
<span class="o">[</span>root@sandbox ~<span class="o">]</span><span class="c1"># unzip access.log.2.zip</span>
Archive:  access.log.2.zip
inflating: access.log.2
<span class="o">[</span>root@sandbox ~<span class="o">]</span><span class="c1"># hdfs dfs -copyFromLocal access.log.2 /user/root/access</span>
</code></pre></div>
</details>
<p><img alt="" src="../images/td2-logs-ambari.PNG" /></p>
<ul>
<li>Take a look at the end of the file in HDFS, using the <code>tail</code> command in HDFS in the terminal.</li>
</ul>
<details class="note"><summary>Output</summary><div class="highlight"><pre><span></span><code><span class="o">[</span>root@sandbox ~<span class="o">]</span><span class="c1"># hdfs dfs -tail access/access.log.2</span>
<span class="m">6</span>.1<span class="p">;</span> WOW64<span class="p">;</span> rv:30.0<span class="o">)</span> Gecko/20100101 Firefox/30.0<span class="s2">&quot;</span>
<span class="s2">64.232.194.248 - - [14/Jun/2014:23:43:32 -0400] &quot;</span>GET /support HTTP/1.1<span class="s2">&quot; 200 887 &quot;</span>-<span class="s2">&quot; &quot;</span>Mozilla/5.0 <span class="o">(</span>Windows NT <span class="m">6</span>.1<span class="p">;</span> rv:30.0<span class="o">)</span> Gecko/20100101 Firefox/30.0<span class="s2">&quot;</span>
<span class="s2">138.9.185.141 - - [14/Jun/2014:23:43:32 -0400] &quot;</span>GET /department/golf HTTP/1.1<span class="s2">&quot; 200 1075 &quot;</span>-<span class="s2">&quot; &quot;</span>Mozilla/5.0 <span class="o">(</span>Windows NT <span class="m">6</span>.3<span class="p">;</span> WOW64<span class="o">)</span> AppleWebKit/537.36 <span class="o">(</span>KHTML, like Gecko<span class="o">)</span> Chrome/35.0.1916.153 Safari/537.36<span class="s2">&quot;</span>
<span class="s2">152.208.225.65 - - [14/Jun/2014:23:43:32 -0400] &quot;</span>GET /department/golf HTTP/1.1<span class="s2">&quot; 200 1358 &quot;</span>-<span class="s2">&quot; &quot;</span>Mozilla/5.0 <span class="o">(</span>Windows NT <span class="m">6</span>.1<span class="o">)</span> AppleWebKit/537.36 <span class="o">(</span>KHTML, like Gecko<span class="o">)</span> Chrome/35.0.1916.153 Safari/537.36<span class="s2">&quot;</span>
<span class="s2">84.246.94.164 - - [14/Jun/2014:23:43:32 -0400] &quot;</span>GET /department/fitness/category/tennis%20<span class="p">&amp;</span>%20racquet HTTP/1.1<span class="s2">&quot; 200 907 &quot;</span>-<span class="s2">&quot; &quot;</span>Mozilla/5.0 <span class="o">(</span>Windows NT <span class="m">6</span>.1<span class="p">;</span> WOW64<span class="p">;</span> rv:30.0<span class="o">)</span> Gecko/20100101 Firefox/30.0<span class="s2">&quot;</span>
<span class="s2">167.228.157.189 - - [14/Jun/2014:23:43:32 -0400] &quot;</span>GET /department/outdoors HTTP/1.1<span class="s2">&quot; 200 2166 &quot;</span>-<span class="s2">&quot; &quot;</span>Mozilla/5.0 <span class="o">(</span>Macintosh<span class="p">;</span> Intel Mac OS X 10_9_3<span class="o">)</span> AppleWebKit/537.36 <span class="o">(</span>KHTML, like Gecko<span class="o">)</span> Chrome/35.0.1916.153 Safari/537.36<span class="s2">&quot;</span>
</code></pre></div>
</details>
<h3 id="build-a-hive-table-over-the-log-file">Build a Hive table over the log file</h3>
<p>In the previous tutorial, we created a Hive table over CSV files using the keywords <code>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</code>. The format of the file in HDFS, which Hive will parse on-demand, must be specified at table creation.</p>
<p>Here we will use a <code>regex</code> to extract all the information we need from the log files. To enable parsing files in HDFS using regex with Hive, we use a specific SerDe (for serializer/deserializer) <code>ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'</code>. </p>
<p>This SERDE is not provided by default, we will need to register a <code>hive-contrib.jar</code> plugin which contains the class. Otherwise you will get <code>Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.contrib.serde2.RegexSerDe not found</code> exceptions.</p>
<p>Let's try this:</p>
<ul>
<li>Open a terminal to your Virtual Machine.</li>
<li>Open a Hive command line: <code>hive</code>.</li>
<li>Add the <code>hive-contrib.jar</code> JAR:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">ADD</span> <span class="n">JAR</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">hdp</span><span class="o">/</span><span class="k">current</span><span class="o">/</span><span class="n">hive</span><span class="o">-</span><span class="n">client</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">hive</span><span class="o">-</span><span class="n">contrib</span><span class="p">.</span><span class="n">jar</span><span class="p">;</span>
</code></pre></div>
<ul>
<li>Create an external Hive table <code>intermediate_access_logs</code> (copy-paste the following command):</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">CREATE</span> <span class="k">EXTERNAL</span> <span class="k">TABLE</span> <span class="n">intermediate_access_logs</span> <span class="p">(</span>
    <span class="n">ip</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">log_date</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="k">method</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">url_site</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">http_version</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">code1</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">code2</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">dash</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">user_agent</span> <span class="n">STRING</span><span class="p">)</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">SERDE</span> <span class="s1">&#39;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&#39;</span>
<span class="k">WITH</span> <span class="n">SERDEPROPERTIES</span> <span class="p">(</span>
    <span class="s1">&#39;input.regex&#39;</span> <span class="o">=</span> <span class="s1">&#39;([^ ]*) - - \\[([^\\]]*)\\] &quot;([^\ ]*) ([^\ ]*) ([^\ ]*)&quot; (\\d*) (\\d*) &quot;([^&quot;]*)&quot; &quot;([^&quot;]*)&quot;&#39;</span><span class="p">,</span>
    <span class="s1">&#39;output.format.string&#39;</span> <span class="o">=</span> <span class="ss">&quot;%1$$s %2$$s %3$$s %4$$s %5$$s %6$$s %7$$s %8$$s %9$$s&quot;</span><span class="p">)</span>
<span class="k">LOCATION</span> <span class="s1">&#39;/user/root/access&#39;</span><span class="p">;</span>
</code></pre></div>
<p>Now whenever you run a SQL query on <code>intermediate_access_logs</code>, Hive will run a MapReduce job by first parsing all files in the <code>/user/root/access</code> with the provided regex, then run your query. </p>
<details class="note"><summary>Output</summary><div class="highlight"><pre><span></span><code>hive&gt; DESCRIBE intermediate_access_logs;
OK
ip                      string                  from deserializer
log_date                string                  from deserializer
method                  string                  from deserializer
url_site                string                  from deserializer
http_version            string                  from deserializer
code1                   string                  from deserializer
code2                   string                  from deserializer
dash                    string                  from deserializer
user_agent              string                  from deserializer
Time taken: 0.997 seconds, Fetched: 9 row(s)
hive&gt; SELECT ip, log_date, user_agent from intermediate_access_logs LIMIT 5;
OK
79.133.215.123  14/Jun/2014:10:30:13 -0400      Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36
162.235.161.200 14/Jun/2014:10:30:13 -0400      Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.76.4 (KHTML, like Gecko) Version/7.0.4 Safari/537.76.4
39.244.91.133   14/Jun/2014:10:30:14 -0400      Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36
150.47.54.136   14/Jun/2014:10:30:14 -0400      Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36
217.89.36.129   14/Jun/2014:10:30:14 -0400      Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0
Time taken: 0.29 seconds, Fetched: 5 row(s)
hive&gt;
</code></pre></div>
</details>
<ul>
<li>Running a query on <code>intermediate_access_logs</code> will parse files with regex every time, which is time consuming. Create a new <code>clean_access_logs</code> table with the output for <code>intermediate_access_logs</code> as content.<ul>
<li>We also optimize the table storage with the <a href="https://orc.apache.org/docs/">ORC format</a>. Using ORC files improves performance when Hive is reading, writing, and processing data.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">clean_access_logs</span> <span class="n">STORED</span> <span class="k">AS</span> <span class="n">ORC</span> <span class="k">AS</span> <span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">intermediate_access_logs</span><span class="p">;</span>
</code></pre></div>
<p>Also build a smaller table so you can experiment on it before running on the full dataset:</p>
<div class="highlight"><pre><span></span><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">sample_access_logs</span> <span class="n">STORED</span> <span class="k">AS</span> <span class="n">ORC</span> <span class="k">AS</span> <span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">clean_access_logs</span> <span class="k">LIMIT</span> <span class="mi">1000</span><span class="p">;</span>
</code></pre></div>
<p>You're now free to work on the <code>clean_access_logs</code> or <code>sample_access_logs</code> tables <img alt="ðŸ˜„" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/1f604.svg" title=":smile:" /></p>
<div class="admonition question">
<p class="admonition-title">SQL questions</p>
<p>Choose 1-2 questions to try:</p>
<ul>
<li>Can you count the number of occurences for each IP address ?</li>
<li>Display how many times each product has been bought</li>
<li>What percentage of IP addresses went to checkout their basket ?</li>
<li>If you case the date as a Date you should be able to build a web journey of an IP address on the website. For all IP adresses that went to checkout, compute the number of products each has bought before.</li>
</ul>
</div>
<h3 id="zeppelin-the-big-data-notebook">Zeppelin, the Big Data notebook</h3>
<p>Apache Zeppelin is a Web-based notebook for interactive data analytics and collaborative documents. You can plugin multiple interpreters to run different Big Data engine inside, by default Hive JDBC and Spark are already configured to run. </p>
<ul>
<li>Open <code>http://localhost:9995</code> for a first peek at Zeppelin.</li>
</ul>
<p><img alt="" src="../images/td2-zeppelin-start.PNG" /></p>
<ul>
<li>Create a new note, write some <a href="https://www.markdownguide.org/">Markdown</a> in the first cell with <code>%md</code> as a first line to choose the Markdown interpreter, and run the cell:</li>
</ul>
<p><img alt="" src="../images/td2-zeppelin-hello.PNG" /></p>
<ul>
<li>See what happens when you toggle the <code>default</code> view to <code>simple</code> and <code>report</code>.</li>
</ul>
<p><img alt="" src="../images/td2-zeppelin-layout.PNG" /></p>
<ul>
<li>Each cell has its own set of settings too. For example, change the width of the first cell to 6:</li>
</ul>
<p><img alt="" src="../images/td2-zeppelin-cell-settings.PNG" /></p>
<ul>
<li>Create a new cell, set the interpreter to <code>Hive JDBC</code> with <code>%jdbc(hive)</code> and run a SQL query against <code>clean_access_logs</code> again.</li>
</ul>
<p><img alt="" src="../images/td2-zeppelin-hive.PNG" /></p>
<ul>
<li>Do you see that the <code>Hive JDBC</code> cell results have a toolbar for displaying graphs? Play with it a bit then try to replicate the following notebook in <code>report</code> view, with a cell for counting the number of occurences for a few ip addresses.</li>
</ul>
<p><img alt="" src="../images/td2-zeppelin-report.PNG" /></p>
<p><strong>Recap</strong></p>
<ul>
<li>We loaded raw Apache logs into HDFS</li>
<li>We parsed them by pointing an Hive Table over the logs with a regex deserializer to parse each line</li>
<li>We saved Hive tables with the extracted info and optimized with ORC.</li>
<li>We built a Zeppelin dashboard with some info on the contents of the log.</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Going back to our objectives</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Structuring Apache logs with Hive and regex</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Ingesting Apache logs into HDFS in realtime with Flume</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Building a data dashboard with Zeppelin</li>
</ul>
</div>
<h2 id="2-generating-logs-with-python">2. Generating logs with Python</h2>
<ul>
<li>Copy gen_logs to VM</li>
<li>Run Python simulation</li>
</ul>
<h2 id="3-ingesting-data-in-hdfs-with-flume">3. Ingesting data in HDFS with Flume</h2>
<ul>
<li>Configure Flume</li>
<li>Output in external Hive table</li>
</ul>
<h2 id="4-dashboarding-with-zeppelin">4. Dashboarding with Zeppelin</h2>
<ul>
<li>TODO</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../td1/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                TD1 - First steps in the Hadoop ecosystem
              </div>
            </div>
          </a>
        
        
          <a href="../td3/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                TD3
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.fd16492e.min.js"></script>
      <script src="../assets/javascripts/bundle.7836ba4d.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>